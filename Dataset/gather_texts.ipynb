{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e5a8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def make_ques_doc(experiment):\n",
    "    human_data_file = pd.read_csv(experiment + \"_counts.csv\")\n",
    "    \n",
    "    with open(\"words_\" + experiment + \".txt\", 'w') as new_file:\n",
    "        new_file.truncate(0)\n",
    "        for i in range(len(human_data_file[\"prompt\"])):\n",
    "            new_file.write(human_data_file[\"type\"][i] + \" \")\n",
    "            new_file.write(human_data_file[\"prompt_w1\"][i] + \" \")\n",
    "            new_file.write(human_data_file[\"prompt_w2\"][i] + \" \")\n",
    "            new_file.write(human_data_file[\"prompt_w3\"][i] + \" \")\n",
    "\n",
    "            for r in human_data_file.columns[5:]:\n",
    "                if human_data_file[r][i] > 0:\n",
    "                    new_file.write(r + \" \")\n",
    "            new_file.write(\"\\n\")\n",
    "\n",
    "        new_file.close()\n",
    "\n",
    "make_ques_doc(\"experiment1a\")\n",
    "make_ques_doc(\"experiment1b\")\n",
    "make_ques_doc(\"experiment1c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3f2c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in csv file\n",
    "#open text file with PG val from first column of csv file\n",
    "\n",
    "# import os\n",
    "# import shutil as sh\n",
    "\n",
    "# def create_text_folder(genre):\n",
    "#     corpus = pd.read_csv(genre + \"_texts.csv\")\n",
    "#     texts = corpus[\"id\"]\n",
    "    \n",
    "#     #path will be changed\n",
    "#     path = genre + \"_docs\"\n",
    "#     # Check whether the specified path exists or not\n",
    "#     isExist = os.path.exists(path)\n",
    "#     if not isExist:\n",
    "#         os.makedirs(path)\n",
    "#     for i in range(len(texts)):\n",
    "#         sh.copy(\"/Users/erikajones/Documents/CS 8803/LSA Project/text_database/gutenberg/data/text/\" + texts[i] + \"_text.txt\", path)\n",
    "\n",
    "# def create_token_folder(genre):\n",
    "#     corpus = pd.read_csv(genre + \"_texts.csv\")\n",
    "#     texts = corpus[\"id\"]\n",
    "    \n",
    "#     #path will be changed\n",
    "#     path = genre + \"_tokens\"\n",
    "#     # Check whether the specified path exists or not\n",
    "#     isExist = os.path.exists(path)\n",
    "#     if not isExist:\n",
    "#         os.makedirs(path)\n",
    "#     for i in range(len(texts)):\n",
    "#         sh.copy(\"/Users/erikajones/Documents/CS 8803/LSA Project/text_database/gutenberg/data/tokens/\" + texts[i] + \"_tokens.txt\", path)\n",
    "        \n",
    "# create_text_folder(\"nonfiction\")\n",
    "# create_text_folder(\"fiction\")\n",
    "# create_text_folder(\"poetry\")\n",
    "\n",
    "# create_token_folder(\"nonfiction\")\n",
    "# create_token_folder(\"fiction\")\n",
    "# create_token_folder(\"poetry\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55e88998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def find_analogy_questions(experiment, genre):\n",
    "        \n",
    "    corpus = pd.read_csv(genre + \"_texts.csv\")\n",
    "    texts = corpus[\"id\"]\n",
    "    \n",
    "    with open(experiment + \"_\" + genre + \"_analogy_questions.txt\", \"w\") as analogies:\n",
    "        analogies.truncate(0)\n",
    "        f = open(\"words_\" + experiment + \".txt\", 'r')\n",
    "        questions = f.readlines()\n",
    "        f.close()\n",
    "        questions_dict = {}\n",
    "        words_in_doc = True\n",
    "        \n",
    "        for row in questions:\n",
    "            row = row.split()\n",
    "            line = row[1:9]\n",
    "            words_in_docs = True\n",
    "            count_sum = 0\n",
    "            word_info = []\n",
    "            \n",
    "            if row[0] not in questions_dict.keys():\n",
    "                questions_dict[row[0]] = []\n",
    "            \n",
    "            for word in line:\n",
    "                \n",
    "                word_count = 0\n",
    "                for i in range(len(texts)):\n",
    "                    with open(genre + \"_tokens/\" + texts[i] + \"_tokens.txt\", \"r\") as file:\n",
    "                        tokens = file.read()\n",
    "                        if word in tokens:\n",
    "                            word_count += 1\n",
    "                        file.close()\n",
    "                if word_count == 0:\n",
    "                    words_in_docs = False\n",
    "                \n",
    "                word_info.append(word) \n",
    "                count_sum += word_count\n",
    "                \n",
    "            while len(word_info) < 8:\n",
    "                word_info += [\"\"]\n",
    "            if words_in_docs:\n",
    "                count_sum = [count_sum]\n",
    "                questions_dict[row[0]].append(count_sum + word_info)\n",
    "                questions_dict[row[0]].sort()\n",
    "            \n",
    "        for i in questions_dict.items():\n",
    "            analogies.write(str(i) + \"\\n\")\n",
    "            \n",
    "        analogies.close()\n",
    "        return questions_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec24bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\"experiment1a\", \"experiment1b\", \"experiment1c\"]\n",
    "genres = [\"nonfiction\", \"fiction\", \"poetry\"]\n",
    "\n",
    "analogy_list = []\n",
    "for e in experiments:\n",
    "    analogy_list_byexp = []\n",
    "    for g in genres:\n",
    "        analogy_list_byexp.append(find_analogy_questions(e,g))\n",
    "    analogy_list.append(analogy_list_byexp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "caaef2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "final_question_dict = {}\n",
    "for exps in analogy_list:\n",
    "    \n",
    "    final_question_dict[experiments[index]] = []\n",
    "    question_dict = {}\n",
    "    \n",
    "    for k in exps[0].keys():\n",
    "        \n",
    "        question_dict[k] = []\n",
    "            \n",
    "        for q_nf in exps[0][k]:\n",
    "            in_fiction = False\n",
    "            in_poetry = False\n",
    "\n",
    "            for q_f in exps[1][k]:\n",
    "                if q_nf[1:4] == q_f[1:4]:\n",
    "                    in_fiction = True\n",
    "\n",
    "            for q_p in exps[2][k]:\n",
    "                if q_nf[1:4] == q_p[1:4]:\n",
    "                    in_poetry = True\n",
    "\n",
    "            if in_fiction and in_poetry:\n",
    "                question_dict[k].append(q_nf[1:9])\n",
    "    final_question_dict[experiments[index]] = question_dict\n",
    "    index += 1\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c54890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "final_dict = {}\n",
    "\n",
    "for exp in experiments:\n",
    "    type_dict = {}\n",
    "    for types in final_question_dict[exp].keys():\n",
    "        type_dict[\"prompt1\"] = [col[0] for col in final_question_dict[exp][types]]\n",
    "        type_dict[\"prompt2\"] = [col[1] for col in final_question_dict[exp][types]]\n",
    "        type_dict[\"prompt3\"] = [col[2] for col in final_question_dict[exp][types]]\n",
    "        type_dict[\"response1\"] = [col[3] for col in final_question_dict[exp][types]]\n",
    "        type_dict[\"response2\"] = [col[4] for col in final_question_dict[exp][types]]\n",
    "        type_dict[\"response3\"] = [col[5] for col in final_question_dict[exp][types]]\n",
    "        type_dict[\"response4\"] = [col[6] for col in final_question_dict[exp][types]]\n",
    "        type_dict[\"response5\"] = [col[7] for col in final_question_dict[exp][types]]\n",
    "        \n",
    "        df = pd.DataFrame(type_dict)\n",
    "        \n",
    "        path = \"analogy_question_selections\"\n",
    "        # Check whether the specified path exists or not\n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist:\n",
    "            os.makedirs(path)\n",
    "        df.to_csv(path + \"/\" + exp + \"_\" + types + \"_questions.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d69878a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "with open(\"fiction_tokens/\" + \"PG10002_tokens.txt\", \"r\") as file:\n",
    "    tokens = file.read()\n",
    "#     print(tokens)\n",
    "    word_count = 0\n",
    "    if \"the\" in tokens:\n",
    "        word_count += 1\n",
    "        print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d150b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
