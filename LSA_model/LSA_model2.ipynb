{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ww7ktLumcfkh",
        "outputId": "329b9be9-b76b-49c3-9336-f18923093d5d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_0xDLy0_bLM8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import glob \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def load_data(genre):\n",
        "  documents_list = []\n",
        "\n",
        "  read_files = glob.glob(\"/content/drive/My Drive/pg_data/\"+genre+\"/*.txt\")\n",
        "\n",
        "  for f in read_files:\n",
        "    with open(f, \"r\") as doc:\n",
        "      text = doc.read().strip()\n",
        "      documents_list.append(text)\n",
        "  return documents_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents_list = load_data(\"fiction\")\n",
        "\n",
        "# Initialize regex tokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+') #\\w+ matches any word character \n",
        "\n",
        "# Vectorize document using TF-IDF\n",
        "tfidf = TfidfVectorizer(lowercase=True,\n",
        "                        stop_words='english',\n",
        "                        ngram_range = (1,1),\n",
        "                        tokenizer = tokenizer.tokenize)\n",
        "\n",
        "# Fit and Transform the documents\n",
        "train_data = tfidf.fit_transform(documents_list)   "
      ],
      "metadata": {
        "id": "ZC_j6Rv6cfHk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of topics or components\n",
        "num_components=5\n",
        "\n",
        "# Create SVD object\n",
        "lsa = TruncatedSVD(n_components=num_components, n_iter=100, random_state=42)\n",
        "\n",
        "# Fit SVD model on data\n",
        "U_SIGMA = lsa.fit_transform(train_data) #returns U * SIGMA\n",
        "\n",
        "# Get Singular values and Components \n",
        "Sigma = lsa.singular_values_ \n",
        "V_transpose = lsa.components_.T\n",
        "\n",
        "U = U_SIGMA / Sigma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15iY1tuaeaCn",
        "outputId": "49102623-15fe-4829-a537-bccc6dab88c4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.0023418  -0.00209438 -0.00485932 -0.00469996 -0.00266133]\n",
            "Topic 0:  ['s', 'said', 'man', 't', 'time']\n",
            "Topic 1:  ['artagnan', 'athos', 'porthos', 'd', 'said']\n",
            "Topic 2:  ['fathom', 'renaldo', 'count', 'ferdinand', 'melvil']\n",
            "Topic 3:  ['mr', 'lydgate', 'dorothea', 'casaubon', 'bulstrode']\n",
            "Topic 4:  ['raskolnikov', 'sonia', 't', 'razumihin', 'dounia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# doc2topic from matrix U\n",
        "doc2topic = {doc: U[idx].tolist() for idx, doc in enumerate(documents_list)}\n",
        "\n",
        "# term2topic from matrix V\n",
        "term2topic = {\n",
        "    term: V_transpose[idx].tolist() for term, idx in tfidf.vocabulary_.items()\n",
        "}"
      ],
      "metadata": {
        "id": "KkgFjwQeehP5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# save mappings\n",
        "with open(\"doc2topic.json\", mode=\"w\") as fp:\n",
        "    json.dump(doc2topic, fp, ensure_ascii=False, indent=4)\n",
        "\n",
        "with open(\"term2topic.json\", mode=\"w\") as fp:\n",
        "    json.dump(term2topic, fp, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "mWl-Z-I5kWbg"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}