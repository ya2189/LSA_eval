{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TxIdjwxsIio",
        "outputId": "a2cab07f-891f-44f1-fec5-958be790c02f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting latent-semantic-analysis\n",
            "  Downloading latent_semantic_analysis-0.1.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.8/dist-packages (from latent-semantic-analysis) (1.21.6)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.8/dist-packages (from latent-semantic-analysis) (6.0)\n",
            "Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.8/dist-packages (from latent-semantic-analysis) (1.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.8/dist-packages (from latent-semantic-analysis) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.5.4 in /usr/local/lib/python3.8/dist-packages (from latent-semantic-analysis) (1.7.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.5->latent-semantic-analysis) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.5->latent-semantic-analysis) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.5->latent-semantic-analysis) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24.2->latent-semantic-analysis) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24.2->latent-semantic-analysis) (3.1.0)\n",
            "Installing collected packages: latent-semantic-analysis\n",
            "Successfully installed latent-semantic-analysis-0.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install latent-semantic-analysis\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"fiction_noCommas.txt\", \"w\") as outfile:\n",
        "#   with open(\"fiction.txt\", \"r\") as infile:\n",
        "#     # remove everything that is not the novel text\n",
        "#     outfile.write(infile.read().replace(',',''))"
      ],
      "metadata": {
        "id": "85-5xNyYig73"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Convert the text file to a csv file.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "read_file = pd.read_csv(r'/content/fiction.txt', names=['text'], sep=',')\n",
        "read_file.to_csv(r'/content/fiction.csv', index=None)"
      ],
      "metadata": {
        "id": "wwZzeQocSe72"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. Performs TF-IDF, which converts a collection of raw documents to a matrix of TF-IDF features.\n",
        "2. Performs dimensionality reduction via Truncated SVD (works with sparse matrices efficiently) \n",
        "** Need to tune parameter \"n_components\" or k, which ranges [2, number of documents]\n",
        "\n",
        "BEFORE RUNNING: \n",
        "1. make sure to put csv file in data folder\n",
        "2. change config.yaml if want to change the dataset, or k value (n_components)\n",
        "\n",
        "OUTPUT:\n",
        "After training the model, the pipeline will return the following files (in the \"models\" folder):\n",
        "\n",
        "model.joblib - sklearn pipeline with LSA (TF-IDF and SVD steps)\n",
        "config.yaml - config that was used to train the model\n",
        "logging.txt - logging file\n",
        "doc2topic.json - document embeddings\n",
        "term2topic.json - term embeddings\n",
        "\"\"\"\n",
        "\n",
        "import latent_semantic_analysis\n",
        "\n",
        "latent_semantic_analysis.train(path_to_config=\"config.yaml\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3tPPleA8ubV",
        "outputId": "0f35f459-b9ca-4a1d-fac2-a7f349dad39f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lsa-train - INFO - Config:\n",
            "\n",
            "seed: 42\n",
            "path_to_save_folder: models\n",
            "\n",
            "# data\n",
            "data:\n",
            "  data_path: data/fiction.csv\n",
            "  sep: ','\n",
            "  text_column: text\n",
            "\n",
            "# tf-idf\n",
            "tf-idf:\n",
            "  lowercase: true\n",
            "  stop_words: 'english'\n",
            "  ngram_range: (1, 1)\n",
            "  max_df: 1.0\n",
            "  min_df: 1\n",
            "\n",
            "# svd\n",
            "svd:\n",
            "  n_components: 10\n",
            "  algorithm: arpack\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lsa-train:Config:\n",
            "\n",
            "seed: 42\n",
            "path_to_save_folder: models\n",
            "\n",
            "# data\n",
            "data:\n",
            "  data_path: data/fiction.csv\n",
            "  sep: ','\n",
            "  text_column: text\n",
            "\n",
            "# tf-idf\n",
            "tf-idf:\n",
            "  lowercase: true\n",
            "  stop_words: 'english'\n",
            "  ngram_range: (1, 1)\n",
            "  max_df: 1.0\n",
            "  min_df: 1\n",
            "\n",
            "# svd\n",
            "svd:\n",
            "  n_components: 10\n",
            "  algorithm: arpack\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lsa-train - INFO - Loading data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lsa-train:Loading data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lsa-train - INFO - Dataset size: 74579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lsa-train:Dataset size: 74579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lsa-train - INFO - Fitting LSA model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lsa-train:Fitting LSA model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Pipeline] ............ (step 1 of 2) Processing tf-idf, total=   0.8s\n",
            "[Pipeline] ............... (step 2 of 2) Processing svd, total=   0.4s\n",
            "lsa-train - INFO - Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lsa-train:Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lsa-train - INFO - TF-IDF number of features: 21162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lsa-train:TF-IDF number of features: 21162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lsa-train - INFO - Saving the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lsa-train:Saving the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lsa-train - INFO - Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lsa-train:Done!\n"
          ]
        }
      ]
    }
  ]
}
