{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_0xDLy0_bLM8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import glob \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from pathlib import Path\n",
        "\n",
        "def load_data(genre):\n",
        "  documents_list = []\n",
        "\n",
        "  path = Path(\"../Dataset/\"+genre+\"_docs/\")\n",
        "  read_files = glob.glob(os.path.join(path,\"*.txt\"))\n",
        "\n",
        "  for f in read_files:\n",
        "    with open(f, \"r\", encoding=\"utf-8\") as doc:\n",
        "      text = doc.read().strip()\n",
        "      documents_list.append(text)\n",
        "  return documents_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def save_embeddings_txt(term2topic, genre, k):\n",
        "    with open(\"term2topic_\"+genre+\"_\"+str(k)+\".txt\", mode=\"wb\") as file:\n",
        "        file.write(pickle.dumps(term2topic)) # use `pickle.loads` to do the reverse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def save_embeddings_json(term2topic, genre, k):\n",
        "    print(\"Saving embeddings ... \")\n",
        "    # save mappings\n",
        "    # with open(\"doc2topic_\"+genre+\"_\"+str(k)+\".json\", mode=\"w\", encoding=\"utf-8\") as fp:\n",
        "    #     json.dump(doc2topic, fp, ensure_ascii=False, indent=4)\n",
        "\n",
        "    with open(\"term2topic_\"+genre+\"_\"+str(k)+\".json\", mode=\"w\", encoding=\"utf-8\") as fp:\n",
        "        json.dump(term2topic, fp, ensure_ascii=False, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def LSA(text_genre, k):\n",
        "    print(\"Training LSA model for genre \"+text_genre+\" and k=\"+str(k))\n",
        "    documents_list = load_data(text_genre)\n",
        "\n",
        "    # Initialize regex tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+') #\\w+ matches any word character \n",
        "\n",
        "    # Vectorize document using TF-IDF\n",
        "    tfidf = TfidfVectorizer(lowercase=True,\n",
        "                            stop_words='english',\n",
        "                            ngram_range = (1,1),\n",
        "                            tokenizer = tokenizer.tokenize)\n",
        "\n",
        "    # Fit and Transform the documents\n",
        "    train_data = tfidf.fit_transform(documents_list)  \n",
        "\n",
        "    # Define the number of topics or components\n",
        "    num_components=k\n",
        "\n",
        "    # Create SVD object\n",
        "    lsa = TruncatedSVD(n_components=num_components, n_iter=100, random_state=42)\n",
        "\n",
        "    # Fit SVD model on data\n",
        "    U_SIGMA = lsa.fit_transform(train_data) #returns U * SIGMA\n",
        "\n",
        "    # Get Singular values and Components \n",
        "    Sigma = lsa.singular_values_ \n",
        "    V_transpose = lsa.components_.T\n",
        "\n",
        "    U = U_SIGMA / Sigma\n",
        "\n",
        "    # doc2topic from matrix U\n",
        "    # doc2topic = {doc: U[idx].tolist() for idx, doc in enumerate(documents_list)}\n",
        "\n",
        "    # term2topic from matrix V\n",
        "    term2topic = {\n",
        "        term: V_transpose[idx] for term, idx in tfidf.vocabulary_.items()\n",
        "    }\n",
        "\n",
        "    save_embeddings_txt(term2topic, text_genre, num_components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSA model for genre poetry and k=2\n"
          ]
        }
      ],
      "source": [
        "#TEST \n",
        "# LSA(\"poetry\", 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSA model for genre fiction and k=2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\yuri_\\anaconda3\\envs\\LSA_eval_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSA model for genre fiction and k=4\n",
            "Training LSA model for genre fiction and k=8\n",
            "Training LSA model for genre fiction and k=16\n",
            "Training LSA model for genre fiction and k=32\n",
            "Training LSA model for genre fiction and k=64\n"
          ]
        }
      ],
      "source": [
        "for k in range(1, 7): \n",
        "    k = 2**k #k ranges from 2 to 64\n",
        "    # LSA(\"poetry\", k)\n",
        "    LSA(\"fiction\", k)\n",
        "    # LSA(\"nonfiction\", k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# USE THIS TO UNPICKLE TEXT FILE\n",
        "\n",
        "# infile = open(\"term2topic_poetry_2.txt\",'rb')\n",
        "# new_dict = pickle.load(infile)\n",
        "# infile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "complete  :  [0.00536542 0.00082792]\n",
            "<class 'str'> <class 'numpy.ndarray'>\n",
            "poetical  :  [0.00261243 0.00082802]\n",
            "<class 'str'> <class 'numpy.ndarray'>\n",
            "works  :  [0.00800802 0.01094901]\n",
            "<class 'str'> <class 'numpy.ndarray'>\n",
            "edgar  :  [0.00152548 0.00181107]\n",
            "<class 'str'> <class 'numpy.ndarray'>\n",
            "allan  :  [0.00152632 0.00140544]\n",
            "<class 'str'> <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "# TEST: check that types are correct \n",
        "\n",
        "# print(type(new_dict))\n",
        "# i = 0\n",
        "# for term, embedding in new_dict.items():\n",
        "#     if i  == 5:\n",
        "#         break\n",
        "#     print(term,\" : \", embedding)\n",
        "#     print(type(term),type(embedding))\n",
        "#     i+=1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "LSA_eval_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "eb4dbace500bde27436d6c62a5916e6de58a43b9494da1bf763028544149a815"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
