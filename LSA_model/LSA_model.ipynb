{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_0xDLy0_bLM8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import glob \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from pathlib import Path\n",
        "\n",
        "def load_data(genre):\n",
        "  documents_list = []\n",
        "\n",
        "  path = Path(\"../Dataset/\"+genre+\"_docs/\")\n",
        "  read_files = glob.glob(os.path.join(path,\"*.txt\"))\n",
        "\n",
        "  for f in read_files:\n",
        "    with open(f, \"r\", encoding=\"utf-8\") as doc:\n",
        "      text = doc.read().strip()\n",
        "      documents_list.append(text)\n",
        "  return documents_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def save_embeddings_txt(term2topic, genre, k):\n",
        "    with open(\"term2topic_\"+genre+\"_\"+str(k)+\".txt\", mode=\"wb\") as file:\n",
        "        file.write(pickle.dumps(term2topic)) # use `pickle.loads` to do the reverse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def save_embeddings_json(term2topic, genre, k):\n",
        "    print(\"Saving embeddings ... \")\n",
        "    # save mappings\n",
        "    # with open(\"doc2topic_\"+genre+\"_\"+str(k)+\".json\", mode=\"w\", encoding=\"utf-8\") as fp:\n",
        "    #     json.dump(doc2topic, fp, ensure_ascii=False, indent=4)\n",
        "\n",
        "    with open(\"term2topic_\"+genre+\"_\"+str(k)+\".json\", mode=\"w\", encoding=\"utf-8\") as fp:\n",
        "        json.dump(term2topic, fp, ensure_ascii=False, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def LSA(text_genre, k):\n",
        "    print(\"Training LSA model for genre \"+text_genre+\" and k=\"+str(k))\n",
        "    documents_list = load_data(text_genre)\n",
        "\n",
        "    # Initialize regex tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+') #\\w+ matches any word character \n",
        "\n",
        "    # Vectorize document using TF-IDF\n",
        "    tfidf = TfidfVectorizer(lowercase=True,\n",
        "                            stop_words='english',\n",
        "                            ngram_range = (1,1),\n",
        "                            tokenizer = tokenizer.tokenize)\n",
        "\n",
        "    # Fit and Transform the documents\n",
        "    train_data = tfidf.fit_transform(documents_list)  \n",
        "\n",
        "    # Define the number of topics or components\n",
        "    num_components=k\n",
        "\n",
        "    # Create SVD object\n",
        "    lsa = TruncatedSVD(n_components=num_components, n_iter=100, random_state=42)\n",
        "\n",
        "    # Fit SVD model on data\n",
        "    U_SIGMA = lsa.fit_transform(train_data) #returns U * SIGMA\n",
        "\n",
        "    # Get Singular values and Components \n",
        "    Sigma = lsa.singular_values_ \n",
        "    V_transpose = lsa.components_.T\n",
        "\n",
        "\n",
        "    U = U_SIGMA / Sigma\n",
        "    # print(\"U:\",U.shape)\n",
        "    # print(\"V^T: \",V_transpose.shape)\n",
        "    # print(\"sigma: \",Sigma)\n",
        "\n",
        "    # doc2topic from matrix U\n",
        "    # doc2topic = {doc: U[idx].tolist() for idx, doc in enumerate(documents_list)}\n",
        "\n",
        "    # term2topic from matrix V\n",
        "    term2topic = {\n",
        "        term: V_transpose[idx] for term, idx in tfidf.vocabulary_.items()\n",
        "    }\n",
        "\n",
        "    save_embeddings_txt(term2topic, text_genre, num_components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSA model for genre poetry and k=2\n"
          ]
        }
      ],
      "source": [
        "#TEST \n",
        "# LSA(\"poetry\", 2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To generate LSA models for in between k values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_LSA_models(k_start, k_stop):\n",
        "    \n",
        "    for k in range(k_start, k_stop):\n",
        "        # LSA(\"poetry\", k)\n",
        "        # LSA(\"fiction\", k)\n",
        "        LSA(\"nonfiction\", k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSA model for genre nonfiction and k=3\n",
            "Training LSA model for genre nonfiction and k=5\n",
            "Training LSA model for genre nonfiction and k=6\n",
            "Training LSA model for genre nonfiction and k=7\n",
            "Training LSA model for genre nonfiction and k=9\n",
            "Training LSA model for genre nonfiction and k=10\n",
            "Training LSA model for genre nonfiction and k=11\n",
            "Training LSA model for genre nonfiction and k=12\n",
            "Training LSA model for genre nonfiction and k=13\n",
            "Training LSA model for genre nonfiction and k=14\n",
            "Training LSA model for genre nonfiction and k=15\n",
            "Training LSA model for genre nonfiction and k=17\n",
            "Training LSA model for genre nonfiction and k=18\n",
            "Training LSA model for genre nonfiction and k=19\n",
            "Training LSA model for genre nonfiction and k=20\n",
            "Training LSA model for genre nonfiction and k=21\n",
            "Training LSA model for genre nonfiction and k=22\n",
            "Training LSA model for genre nonfiction and k=23\n",
            "Training LSA model for genre nonfiction and k=24\n",
            "Training LSA model for genre nonfiction and k=25\n",
            "Training LSA model for genre nonfiction and k=26\n",
            "Training LSA model for genre nonfiction and k=27\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [19], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m generate_LSA_models(\u001b[39m5\u001b[39m, \u001b[39m8\u001b[39m) \u001b[39m#5-7\u001b[39;00m\n\u001b[0;32m      3\u001b[0m generate_LSA_models(\u001b[39m9\u001b[39m, \u001b[39m16\u001b[39m) \u001b[39m#9-15\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m generate_LSA_models(\u001b[39m17\u001b[39;49m, \u001b[39m32\u001b[39;49m) \u001b[39m#17-31\u001b[39;00m\n\u001b[0;32m      5\u001b[0m generate_LSA_models(\u001b[39m33\u001b[39m, \u001b[39m51\u001b[39m)\n",
            "Cell \u001b[1;32mIn [18], line 6\u001b[0m, in \u001b[0;36mgenerate_LSA_models\u001b[1;34m(k_start, k_stop)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_LSA_models\u001b[39m(k_start, k_stop):\n\u001b[0;32m      3\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(k_start, k_stop):\n\u001b[0;32m      4\u001b[0m         \u001b[39m# LSA(\"poetry\", k)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         \u001b[39m# LSA(\"fiction\", k)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m         LSA(\u001b[39m\"\u001b[39;49m\u001b[39mnonfiction\u001b[39;49m\u001b[39m\"\u001b[39;49m, k)\n",
            "Cell \u001b[1;32mIn [5], line 15\u001b[0m, in \u001b[0;36mLSA\u001b[1;34m(text_genre, k)\u001b[0m\n\u001b[0;32m      9\u001b[0m tfidf \u001b[39m=\u001b[39m TfidfVectorizer(lowercase\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m                         stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     11\u001b[0m                         ngram_range \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m),\n\u001b[0;32m     12\u001b[0m                         tokenizer \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtokenize)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Fit and Transform the documents\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m train_data \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39;49mfit_transform(documents_list)  \n\u001b[0;32m     17\u001b[0m \u001b[39m# Define the number of topics or components\u001b[39;00m\n\u001b[0;32m     18\u001b[0m num_components\u001b[39m=\u001b[39mk\n",
            "File \u001b[1;32mc:\\Users\\yuri_\\anaconda3\\envs\\LSA_eval_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2121\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2114\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2116\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2117\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2118\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2119\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2120\u001b[0m )\n\u001b[1;32m-> 2121\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2123\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2124\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\yuri_\\anaconda3\\envs\\LSA_eval_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1377\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1369\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1370\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1374\u001b[0m             )\n\u001b[0;32m   1375\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1377\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1380\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\yuri_\\anaconda3\\envs\\LSA_eval_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1264\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1263\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1264\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1265\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1266\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
            "File \u001b[1;32mc:\\Users\\yuri_\\anaconda3\\envs\\LSA_eval_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:116\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m ngrams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 116\u001b[0m         doc \u001b[39m=\u001b[39m ngrams(doc, stop_words)\n\u001b[0;32m    117\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m         doc \u001b[39m=\u001b[39m ngrams(doc)\n",
            "File \u001b[1;32mc:\\Users\\yuri_\\anaconda3\\envs\\LSA_eval_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:249\u001b[0m, in \u001b[0;36m_VectorizerMixin._word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39m# handle stop words\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 249\u001b[0m     tokens \u001b[39m=\u001b[39m [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m w \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m    251\u001b[0m \u001b[39m# handle token n-grams\u001b[39;00m\n\u001b[0;32m    252\u001b[0m min_n, max_n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngram_range\n",
            "File \u001b[1;32mc:\\Users\\yuri_\\anaconda3\\envs\\LSA_eval_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:249\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39m# handle stop words\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 249\u001b[0m     tokens \u001b[39m=\u001b[39m [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m w \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m stop_words]\n\u001b[0;32m    251\u001b[0m \u001b[39m# handle token n-grams\u001b[39;00m\n\u001b[0;32m    252\u001b[0m min_n, max_n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngram_range\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "generate_LSA_models(3, 4) #3\n",
        "generate_LSA_models(5, 8) #5-7\n",
        "generate_LSA_models(9, 16) #9-15\n",
        "generate_LSA_models(17, 32) #17-31\n",
        "generate_LSA_models(33, 51) #33-50"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To generate LSA models for k = [2,4,8,16,32,64]:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSA model for genre fiction and k=2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\yuri_\\anaconda3\\envs\\LSA_eval_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSA model for genre fiction and k=4\n",
            "Training LSA model for genre fiction and k=8\n",
            "Training LSA model for genre fiction and k=16\n",
            "Training LSA model for genre fiction and k=32\n",
            "Training LSA model for genre fiction and k=64\n"
          ]
        }
      ],
      "source": [
        "for k in range(1, 7): \n",
        "    k = 2**k #k ranges from 2 to 64\n",
        "    # LSA(\"poetry\", k)\n",
        "    # LSA(\"fiction\", k)\n",
        "    LSA(\"nonfiction\", k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# USE THIS TO UNPICKLE TEXT FILE\n",
        "\n",
        "# infile = open(\"term2topic_poetry_2.txt\",'rb')\n",
        "# new_dict = pickle.load(infile)\n",
        "# infile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "complete  :  [0.00536542 0.00082792]\n",
            "<class 'str'> <class 'numpy.ndarray'>\n",
            "poetical  :  [0.00261243 0.00082802]\n",
            "<class 'str'> <class 'numpy.ndarray'>\n",
            "works  :  [0.00800802 0.01094901]\n",
            "<class 'str'> <class 'numpy.ndarray'>\n",
            "edgar  :  [0.00152548 0.00181107]\n",
            "<class 'str'> <class 'numpy.ndarray'>\n",
            "allan  :  [0.00152632 0.00140544]\n",
            "<class 'str'> <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "# TEST: check that types are correct \n",
        "\n",
        "# print(type(new_dict))\n",
        "# i = 0\n",
        "# for term, embedding in new_dict.items():\n",
        "#     if i  == 5:\n",
        "#         break\n",
        "#     print(term,\" : \", embedding)\n",
        "#     print(type(term),type(embedding))\n",
        "#     i+=1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "LSA_eval_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "eb4dbace500bde27436d6c62a5916e6de58a43b9494da1bf763028544149a815"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
