{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_0xDLy0_bLM8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import glob \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from pathlib import Path\n",
        "\n",
        "def load_data(genre):\n",
        "  documents_list = []\n",
        "\n",
        "  path = Path(\"../Dataset/\"+genre+\"_docs/\")\n",
        "  read_files = glob.glob(os.path.join(path,\"*.txt\"))\n",
        "\n",
        "  for f in read_files:\n",
        "    with open(f, \"r\", encoding=\"utf-8\") as doc:\n",
        "      text = doc.read().strip()\n",
        "      documents_list.append(text)\n",
        "  return documents_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def save_embeddings_txt(term2topic, genre, k):\n",
        "    with open(\"term2topic_\"+genre+\"_\"+str(k)+\".txt\", mode=\"wb\") as file:\n",
        "        file.write(pickle.dumps(term2topic)) # use `pickle.loads` to do the reverse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def save_embeddings_json(term2topic, genre, k):\n",
        "    print(\"Saving embeddings ... \")\n",
        "    # save mappings\n",
        "    # with open(\"doc2topic_\"+genre+\"_\"+str(k)+\".json\", mode=\"w\", encoding=\"utf-8\") as fp:\n",
        "    #     json.dump(doc2topic, fp, ensure_ascii=False, indent=4)\n",
        "\n",
        "    with open(\"term2topic_\"+genre+\"_\"+str(k)+\".json\", mode=\"w\", encoding=\"utf-8\") as fp:\n",
        "        json.dump(term2topic, fp, ensure_ascii=False, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def LSA(text_genre, k):\n",
        "    print(\"Training LSA model for genre \"+text_genre+\" and k=\"+str(k))\n",
        "    documents_list = load_data(text_genre)\n",
        "\n",
        "    # Initialize regex tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+') #\\w+ matches any word character \n",
        "\n",
        "    # Vectorize document using TF-IDF\n",
        "    tfidf = TfidfVectorizer(lowercase=True,\n",
        "                            stop_words='english',\n",
        "                            ngram_range = (1,1),\n",
        "                            tokenizer = tokenizer.tokenize)\n",
        "\n",
        "    # Fit and Transform the documents\n",
        "    train_data = tfidf.fit_transform(documents_list)  \n",
        "\n",
        "    # Define the number of topics or components\n",
        "    num_components=k\n",
        "\n",
        "    # Create SVD object\n",
        "    lsa = TruncatedSVD(n_components=num_components, n_iter=100, random_state=42)\n",
        "\n",
        "    # Fit SVD model on data\n",
        "    U_SIGMA = lsa.fit_transform(train_data) #returns U * SIGMA\n",
        "\n",
        "    # Get Singular values and Components \n",
        "    Sigma = lsa.singular_values_ \n",
        "    V_transpose = lsa.components_.T\n",
        "\n",
        "\n",
        "    U = U_SIGMA / Sigma\n",
        "    # print(\"U:\",U.shape)\n",
        "    # print(\"V^T: \",V_transpose.shape)\n",
        "    # print(\"sigma: \",Sigma)\n",
        "\n",
        "    # doc2topic from matrix U\n",
        "    # doc2topic = {doc: U[idx].tolist() for idx, doc in enumerate(documents_list)}\n",
        "\n",
        "    # term2topic from matrix V\n",
        "    term2topic = {\n",
        "        term: V_transpose[idx] for term, idx in tfidf.vocabulary_.items()\n",
        "    }\n",
        "\n",
        "    save_embeddings_txt(term2topic, text_genre, num_components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSA model for genre poetry and k=2\n"
          ]
        }
      ],
      "source": [
        "#TEST \n",
        "# LSA(\"poetry\", 2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To generate LSA models for in between k values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_LSA_models(k_start, k_stop):\n",
        "    \n",
        "    for k in range(k_start, k_stop):\n",
        "        LSA(\"poetry\", k)\n",
        "        # LSA(\"fiction\", k)\n",
        "        # LSA(\"nonfiction\", k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSA model for genre poetry and k=39\n",
            "Training LSA model for genre poetry and k=40\n",
            "Training LSA model for genre poetry and k=41\n",
            "Training LSA model for genre poetry and k=42\n",
            "Training LSA model for genre poetry and k=43\n",
            "Training LSA model for genre poetry and k=44\n",
            "Training LSA model for genre poetry and k=45\n",
            "Training LSA model for genre poetry and k=46\n",
            "Training LSA model for genre poetry and k=47\n",
            "Training LSA model for genre poetry and k=48\n",
            "Training LSA model for genre poetry and k=49\n",
            "Training LSA model for genre poetry and k=50\n",
            "Training LSA model for genre poetry and k=51\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [11], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# generate_LSA_models(3, 4) #3\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# generate_LSA_models(5, 8) #5-7\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# generate_LSA_models(9, 16) #9-15\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# generate_LSA_models(17, 32) #17-31\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m generate_LSA_models(\u001b[39m39\u001b[39;49m, \u001b[39m64\u001b[39;49m)\n",
            "Cell \u001b[1;32mIn [6], line 4\u001b[0m, in \u001b[0;36mgenerate_LSA_models\u001b[1;34m(k_start, k_stop)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_LSA_models\u001b[39m(k_start, k_stop):\n\u001b[0;32m      3\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(k_start, k_stop):\n\u001b[1;32m----> 4\u001b[0m         LSA(\u001b[39m\"\u001b[39;49m\u001b[39mpoetry\u001b[39;49m\u001b[39m\"\u001b[39;49m, k)\n",
            "Cell \u001b[1;32mIn [5], line 24\u001b[0m, in \u001b[0;36mLSA\u001b[1;34m(text_genre, k)\u001b[0m\n\u001b[0;32m     21\u001b[0m lsa \u001b[39m=\u001b[39m TruncatedSVD(n_components\u001b[39m=\u001b[39mnum_components, n_iter\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[39m# Fit SVD model on data\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m U_SIGMA \u001b[39m=\u001b[39m lsa\u001b[39m.\u001b[39;49mfit_transform(train_data) \u001b[39m#returns U * SIGMA\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m# Get Singular values and Components \u001b[39;00m\n\u001b[0;32m     27\u001b[0m Sigma \u001b[39m=\u001b[39m lsa\u001b[39m.\u001b[39msingular_values_ \n",
            "File \u001b[1;32mc:\\Users\\yuri_\\anaconda3\\envs\\LSA_eval_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\yuri_\\anaconda3\\envs\\LSA_eval_env\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:241\u001b[0m, in \u001b[0;36mTruncatedSVD.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39m>\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[0;32m    237\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    238\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mn_components(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components\u001b[39m}\u001b[39;00m\u001b[39m) must be <=\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m n_features(\u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m         )\n\u001b[1;32m--> 241\u001b[0m     U, Sigma, VT \u001b[39m=\u001b[39m randomized_svd(\n\u001b[0;32m    242\u001b[0m         X,\n\u001b[0;32m    243\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_components,\n\u001b[0;32m    244\u001b[0m         n_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter,\n\u001b[0;32m    245\u001b[0m         n_oversamples\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_oversamples,\n\u001b[0;32m    246\u001b[0m         power_iteration_normalizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpower_iteration_normalizer,\n\u001b[0;32m    247\u001b[0m         random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[0;32m    248\u001b[0m     )\n\u001b[0;32m    250\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponents_ \u001b[39m=\u001b[39m VT\n\u001b[0;32m    252\u001b[0m \u001b[39m# As a result of the SVD approximation error on X ~ U @ Sigma @ V.T,\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[39m# X @ V is not the same as U @ Sigma\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\yuri_\\anaconda3\\envs\\LSA_eval_env\\lib\\site-packages\\sklearn\\utils\\extmath.py:446\u001b[0m, in \u001b[0;36mrandomized_svd\u001b[1;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m transpose:\n\u001b[0;32m    443\u001b[0m     \u001b[39m# this implementation is a bit faster with smaller shape[1]\u001b[39;00m\n\u001b[0;32m    444\u001b[0m     M \u001b[39m=\u001b[39m M\u001b[39m.\u001b[39mT\n\u001b[1;32m--> 446\u001b[0m Q \u001b[39m=\u001b[39m randomized_range_finder(\n\u001b[0;32m    447\u001b[0m     M,\n\u001b[0;32m    448\u001b[0m     size\u001b[39m=\u001b[39;49mn_random,\n\u001b[0;32m    449\u001b[0m     n_iter\u001b[39m=\u001b[39;49mn_iter,\n\u001b[0;32m    450\u001b[0m     power_iteration_normalizer\u001b[39m=\u001b[39;49mpower_iteration_normalizer,\n\u001b[0;32m    451\u001b[0m     random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[0;32m    452\u001b[0m )\n\u001b[0;32m    454\u001b[0m \u001b[39m# project M to the (k + p) dimensional space using the basis vectors\u001b[39;00m\n\u001b[0;32m    455\u001b[0m B \u001b[39m=\u001b[39m safe_sparse_dot(Q\u001b[39m.\u001b[39mT, M)\n",
            "File \u001b[1;32mc:\\Users\\yuri_\\anaconda3\\envs\\LSA_eval_env\\lib\\site-packages\\sklearn\\utils\\extmath.py:274\u001b[0m, in \u001b[0;36mrandomized_range_finder\u001b[1;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[0;32m    272\u001b[0m     Q \u001b[39m=\u001b[39m safe_sparse_dot(A\u001b[39m.\u001b[39mT, Q)\n\u001b[0;32m    273\u001b[0m \u001b[39melif\u001b[39;00m power_iteration_normalizer \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLU\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 274\u001b[0m     Q, _ \u001b[39m=\u001b[39m linalg\u001b[39m.\u001b[39mlu(safe_sparse_dot(A, Q), permute_l\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    275\u001b[0m     Q, _ \u001b[39m=\u001b[39m linalg\u001b[39m.\u001b[39mlu(safe_sparse_dot(A\u001b[39m.\u001b[39mT, Q), permute_l\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    276\u001b[0m \u001b[39melif\u001b[39;00m power_iteration_normalizer \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mQR\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\yuri_\\anaconda3\\envs\\LSA_eval_env\\lib\\site-packages\\sklearn\\utils\\extmath.py:189\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    187\u001b[0m         ret \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(a, b)\n\u001b[0;32m    188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39;49m b\n\u001b[0;32m    191\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    192\u001b[0m     sparse\u001b[39m.\u001b[39missparse(a)\n\u001b[0;32m    193\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[0;32m    194\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[0;32m    195\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    196\u001b[0m ):\n\u001b[0;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39mtoarray()\n",
            "File \u001b[1;32mc:\\Users\\yuri_\\anaconda3\\envs\\LSA_eval_env\\lib\\site-packages\\scipy\\sparse\\_base.py:630\u001b[0m, in \u001b[0;36mspmatrix.__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    628\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mScalar operands are not allowed, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    629\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39muse \u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 630\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_dispatch(other)\n",
            "File \u001b[1;32mc:\\Users\\yuri_\\anaconda3\\envs\\LSA_eval_env\\lib\\site-packages\\scipy\\sparse\\_base.py:532\u001b[0m, in \u001b[0;36mspmatrix._mul_dispatch\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mul_vector(other\u001b[39m.\u001b[39mravel())\u001b[39m.\u001b[39mreshape(M, \u001b[39m1\u001b[39m)\n\u001b[0;32m    531\u001b[0m     \u001b[39melif\u001b[39;00m other\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m other\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m N:\n\u001b[1;32m--> 532\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_multivector(other)\n\u001b[0;32m    534\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    535\u001b[0m     \u001b[39m# scalar value\u001b[39;00m\n\u001b[0;32m    536\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mul_scalar(other)\n",
            "File \u001b[1;32mc:\\Users\\yuri_\\anaconda3\\envs\\LSA_eval_env\\lib\\site-packages\\scipy\\sparse\\_compressed.py:502\u001b[0m, in \u001b[0;36m_cs_matrix._mul_multivector\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[39m# csr_matvecs or csc_matvecs\u001b[39;00m\n\u001b[0;32m    501\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(_sparsetools, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_matvecs\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 502\u001b[0m fn(M, N, n_vecs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindptr, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata,\n\u001b[0;32m    503\u001b[0m    other\u001b[39m.\u001b[39;49mravel(), result\u001b[39m.\u001b[39;49mravel())\n\u001b[0;32m    505\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# generate_LSA_models(3, 4) #3\n",
        "# generate_LSA_models(5, 8) #5-7\n",
        "# generate_LSA_models(9, 16) #9-15\n",
        "# generate_LSA_models(17, 32) #17-31\n",
        "generate_LSA_models(39, 64) #33-63"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To generate LSA models for k = [2,4,8,16,32,64]:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSA model for genre fiction and k=2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\yuri_\\anaconda3\\envs\\LSA_eval_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSA model for genre fiction and k=4\n",
            "Training LSA model for genre fiction and k=8\n",
            "Training LSA model for genre fiction and k=16\n",
            "Training LSA model for genre fiction and k=32\n",
            "Training LSA model for genre fiction and k=64\n"
          ]
        }
      ],
      "source": [
        "for k in range(1, 7): \n",
        "    k = 2**k #k ranges from 2 to 64\n",
        "    # LSA(\"poetry\", k)\n",
        "    LSA(\"fiction\", k)\n",
        "    # LSA(\"nonfiction\", k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# USE THIS TO UNPICKLE TEXT FILE\n",
        "\n",
        "# infile = open(\"term2topic_poetry_2.txt\",'rb')\n",
        "# new_dict = pickle.load(infile)\n",
        "# infile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "complete  :  [0.00536542 0.00082792]\n",
            "<class 'str'> <class 'numpy.ndarray'>\n",
            "poetical  :  [0.00261243 0.00082802]\n",
            "<class 'str'> <class 'numpy.ndarray'>\n",
            "works  :  [0.00800802 0.01094901]\n",
            "<class 'str'> <class 'numpy.ndarray'>\n",
            "edgar  :  [0.00152548 0.00181107]\n",
            "<class 'str'> <class 'numpy.ndarray'>\n",
            "allan  :  [0.00152632 0.00140544]\n",
            "<class 'str'> <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "# TEST: check that types are correct \n",
        "\n",
        "# print(type(new_dict))\n",
        "# i = 0\n",
        "# for term, embedding in new_dict.items():\n",
        "#     if i  == 5:\n",
        "#         break\n",
        "#     print(term,\" : \", embedding)\n",
        "#     print(type(term),type(embedding))\n",
        "#     i+=1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "LSA_eval_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "eb4dbace500bde27436d6c62a5916e6de58a43b9494da1bf763028544149a815"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
